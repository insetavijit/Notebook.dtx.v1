{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Enterprise Technical Analysis Framework\n",
    "Combines fluent interface with TALib integration for comprehensive financial analysis\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "from typing import Union, Optional, Dict, List, Callable, Any, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import re\n",
    "from functools import lru_cache, wraps\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af589cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build full path to CSV\n",
    "csv_path = \"../src/xauusdm15.filtered.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e31a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Enterprise Technical Analysis Framework ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 12:47:58,131 - INFO - Initialized analyzer with DataFrame shape: (1000, 6)\n",
      "2025-09-18 12:47:58,379 - INFO - Memory usage: 0.05MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo 1: Fluent Interface with TALib Integration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 12:48:21,960 - INFO - ✓ Added indicator EMA_21 in 23.5711s\n",
      "2025-09-18 12:48:22,888 - INFO - ✓ Added indicator RSI_14 in 0.8424s\n",
      "2025-09-18 12:48:23,129 - INFO - ✓ Added indicator EMA_50 in 0.2394s\n",
      "2025-09-18 12:48:23,146 - INFO - ✓ Close above EMA_21 -> Close_above_EMA_21 (0.0151s)\n",
      "2025-09-18 12:48:23,150 - INFO - ✓ RSI_14 below 70.0 -> RSI_14_below_70_0 (0.0022s)\n",
      "2025-09-18 12:48:23,157 - INFO - ✓ Volume above 1200000.0 -> Volume_above_1200000_0 (0.0049s)\n",
      "2025-09-18 12:48:23,164 - INFO - ✓ Close crossed_up EMA_50 -> Close_crossed_up_EMA_50 (0.0060s)\n",
      "2025-09-18 12:48:23,300 - INFO - ✓ Added indicator EMA_21 in 0.0048s\n",
      "2025-09-18 12:48:23,301 - INFO - Successfully added EMA_21\n",
      "2025-09-18 12:48:23,304 - INFO - ✓ Added indicator EMA_50 in 0.0027s\n",
      "2025-09-18 12:48:23,306 - INFO - Successfully added EMA_50\n",
      "2025-09-18 12:48:23,310 - INFO - ✓ Added indicator RSI_14 in 0.0029s\n",
      "2025-09-18 12:48:23,311 - INFO - Successfully added RSI_14\n",
      "2025-09-18 12:48:23,318 - INFO - ✓ Added indicator MACD in 0.0056s\n",
      "2025-09-18 12:48:23,319 - INFO - Successfully added MACD\n",
      "2025-09-18 12:48:23,329 - INFO - ✓ Added indicator ADX_14 in 0.0094s\n",
      "2025-09-18 12:48:23,330 - INFO - Successfully added ADX_14\n",
      "2025-09-18 12:48:23,334 - INFO - ✓ Close above EMA_21 -> bullish_trend (0.0022s)\n",
      "2025-09-18 12:48:23,338 - INFO - ✓ EMA_21 above EMA_50 -> ema_bullish (0.0030s)\n",
      "2025-09-18 12:48:23,342 - INFO - ✓ RSI_14 below 70 -> rsi_not_overbought (0.0019s)\n",
      "2025-09-18 12:48:23,344 - INFO - ✓ RSI_14 above 30 -> rsi_not_oversold (0.0013s)\n",
      "2025-09-18 12:48:23,347 - INFO - Created trend following signal with 3 components\n",
      "2025-09-18 12:48:23,376 - INFO - Memory optimization saved 0.01MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Operation                   Column  Success  \\\n",
      "0        ADD_INDICATOR_EMA                   EMA_21     True   \n",
      "1        ADD_INDICATOR_RSI                   RSI_14     True   \n",
      "2        ADD_INDICATOR_EMA                   EMA_50     True   \n",
      "3       Close above EMA_21       Close_above_EMA_21     True   \n",
      "4        RSI_14 below 70.0        RSI_14_below_70_0     True   \n",
      "5   Volume above 1200000.0   Volume_above_1200000_0     True   \n",
      "6  Close crossed_up EMA_50  Close_crossed_up_EMA_50     True   \n",
      "\n",
      "   Execution_Time_ms  Active_Signals  Signal_Ratio  \\\n",
      "0           23571.06    97988.785116   9798.878512   \n",
      "1             842.44    50583.578095   5058.357809   \n",
      "2             239.41    95084.546366   9508.454637   \n",
      "3              15.08      496.000000     49.600000   \n",
      "4               2.23      905.000000     90.500000   \n",
      "5               4.86        0.000000      0.000000   \n",
      "6               6.03       45.000000      4.500000   \n",
      "\n",
      "                             Message  \n",
      "0       Indicator added successfully  \n",
      "1       Indicator added successfully  \n",
      "2       Indicator added successfully  \n",
      "3  Comparison completed successfully  \n",
      "4  Comparison completed successfully  \n",
      "5  Comparison completed successfully  \n",
      "6  Comparison completed successfully  \n",
      "\n",
      "Demo 2: Performance Report\n",
      "total_operations: 7\n",
      "successful_operations: 7\n",
      "success_rate: 100.0\n",
      "total_execution_time: 24.68110650500148\n",
      "average_execution_time: 3.5258723578573545\n",
      "memory_usage_mb: 0.07260513305664062\n",
      "dataframe_shape: (1000, 13)\n",
      "generated_columns: 7\n",
      "\n",
      "Demo 3: Advanced Signal Generation\n",
      "Backtest Results: {'total_signals': 277, 'avg_return': np.float64(7.374960685962802e-05), 'win_rate': np.float64(0.5379061371841155), 'best_return': np.float64(0.0014346004650732179), 'worst_return': np.float64(-0.0013521983080442665), 'total_return': np.float64(0.02042864110011696)}\n",
      "\n",
      "Demo 5: Memory Optimization\n",
      "Memory usage: 0.10MB -> 0.09MB\n",
      "\n",
      "Final DataFrame shape: (1000, 20)\n",
      "Generated columns: 14\n",
      "\n",
      "Available TALib indicators: 161\n",
      "Example indicators: ['ACCBANDS', 'ACOS', 'AD', 'ADD', 'ADOSC', 'ADX', 'ADXR', 'APO', 'AROON', 'AROONOSC']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Enterprise Technical Analysis Framework\n",
    "Combines fluent interface with TALib integration for comprehensive financial analysis\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "from typing import Union, Optional, Dict, List, Callable, Any, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import re\n",
    "from functools import lru_cache, wraps\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# %%\n",
    "class ComparisonType(Enum):\n",
    "    \"\"\"Enumeration of supported comparison operations\"\"\"\n",
    "    ABOVE = \"above\"\n",
    "    BELOW = \"below\"\n",
    "    CROSSED_UP = \"crossed_up\"\n",
    "    CROSSED_DOWN = \"crossed_dn\"\n",
    "    EQUALS = \"equals\"\n",
    "    GREATER_EQUAL = \"greater_equal\"\n",
    "    LESS_EQUAL = \"less_equal\"\n",
    "\n",
    "class IndicatorType(Enum):\n",
    "    \"\"\"Enumeration of TALib indicator categories\"\"\"\n",
    "    OVERLAP = \"overlap\"\n",
    "    MOMENTUM = \"momentum\"\n",
    "    VOLUME = \"volume\"\n",
    "    VOLATILITY = \"volatility\"\n",
    "    PRICE = \"price\"\n",
    "    CYCLE = \"cycle\"\n",
    "    PATTERN = \"pattern\"\n",
    "\n",
    "# %%\n",
    "@dataclass\n",
    "class AnalysisResult:\n",
    "    \"\"\"Enhanced data class to encapsulate analysis results\"\"\"\n",
    "    column_name: str\n",
    "    operation: str\n",
    "    success: bool\n",
    "    message: str = \"\"\n",
    "    data: Optional[pd.Series] = None\n",
    "    execution_time: float = 0.0\n",
    "    memory_usage: int = 0\n",
    "\n",
    "@dataclass\n",
    "class IndicatorConfig:\n",
    "    \"\"\"Configuration for technical indicators\"\"\"\n",
    "    name: str\n",
    "    period: Optional[int] = None\n",
    "    fast_period: Optional[int] = None\n",
    "    slow_period: Optional[int] = None\n",
    "    signal_period: Optional[int] = None\n",
    "    parameters: Dict[str, Any] = field(default_factory=dict)\n",
    "    source_column: str = \"Close\"\n",
    "\n",
    "class TAException(Exception):\n",
    "    \"\"\"Enhanced exception for Technical Analysis operations\"\"\"\n",
    "    def __init__(self, message: str, error_code: str = \"GENERAL\"):\n",
    "        self.error_code = error_code\n",
    "        super().__init__(message)\n",
    "\n",
    "# %%\n",
    "class PerformanceProfiler:\n",
    "    \"\"\"Performance monitoring and optimization utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def profile_execution(func: Callable) -> Callable:\n",
    "        \"\"\"Decorator to profile function execution time and memory\"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            import time\n",
    "            import tracemalloc\n",
    "\n",
    "            tracemalloc.start()\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                return result\n",
    "            finally:\n",
    "                execution_time = time.perf_counter() - start_time\n",
    "                current, peak = tracemalloc.get_traced_memory()\n",
    "                tracemalloc.stop()\n",
    "\n",
    "                logger.debug(f\"{func.__name__} executed in {execution_time:.4f}s, \"\n",
    "                           f\"memory: {current / 1024 / 1024:.2f}MB\")\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    @staticmethod\n",
    "    @contextmanager\n",
    "    def memory_efficient_processing():\n",
    "        \"\"\"Context manager for memory-efficient processing\"\"\"\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "# %%\n",
    "class DataValidator:\n",
    "    \"\"\"Enhanced data validation with performance optimization\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=128)\n",
    "    def validate_column_exists(df_shape: Tuple[int, int], columns_tuple: Tuple[str, ...],\n",
    "                              column: str) -> bool:\n",
    "        \"\"\"Cached column existence validation\"\"\"\n",
    "        if column not in columns_tuple:\n",
    "            raise TAException(f\"Column '{column}' not found. Available: {list(columns_tuple)}\",\n",
    "                            \"COLUMN_NOT_FOUND\")\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_numeric_column(df: pd.DataFrame, column: str) -> bool:\n",
    "        \"\"\"Enhanced numeric column validation with performance optimization\"\"\"\n",
    "        DataValidator.validate_column_exists(df.shape, tuple(df.columns), column)\n",
    "\n",
    "        if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "            raise TAException(f\"Column '{column}' must be numeric, got {df[column].dtype}\",\n",
    "                            \"INVALID_DTYPE\")\n",
    "\n",
    "        # Check for excessive NaN values\n",
    "        nan_ratio = df[column].isna().sum() / len(df)\n",
    "        if nan_ratio > 0.5:\n",
    "            logger.warning(f\"Column '{column}' has {nan_ratio:.1%} NaN values\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_ohlcv_data(df: pd.DataFrame) -> Dict[str, bool]:\n",
    "        \"\"\"Validate OHLCV data structure\"\"\"\n",
    "        required_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        optional_cols = ['Volume']\n",
    "\n",
    "        validation_results = {}\n",
    "\n",
    "        for col in required_cols:\n",
    "            try:\n",
    "                DataValidator.validate_numeric_column(df, col)\n",
    "                validation_results[col] = True\n",
    "            except TAException:\n",
    "                validation_results[col] = False\n",
    "                logger.warning(f\"Required OHLCV column '{col}' is invalid or missing\")\n",
    "\n",
    "        for col in optional_cols:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    DataValidator.validate_numeric_column(df, col)\n",
    "                    validation_results[col] = True\n",
    "                except TAException:\n",
    "                    validation_results[col] = False\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "# %%\n",
    "class TALibIndicatorEngine:\n",
    "    \"\"\"High-performance TALib indicator calculation engine\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._indicator_cache = {}\n",
    "        self._available_indicators = self._get_available_indicators()\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=1)\n",
    "    def _get_available_indicators() -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Cache available TALib indicators with their metadata\"\"\"\n",
    "        indicators = {}\n",
    "\n",
    "        for func_name in dir(talib):\n",
    "            if func_name.isupper() and hasattr(talib, func_name):\n",
    "                func = getattr(talib, func_name)\n",
    "                if callable(func):\n",
    "                    try:\n",
    "                        info = talib.abstract.Function(func_name).info\n",
    "                        indicators[func_name] = {\n",
    "                            'function': func,\n",
    "                            'info': info,\n",
    "                            'inputs': info.get('input_names', []),\n",
    "                            'parameters': info.get('parameters', {}),\n",
    "                            'outputs': info.get('output_names', [])\n",
    "                        }\n",
    "                    except:\n",
    "                        # Fallback for indicators without abstract info\n",
    "                        indicators[func_name] = {\n",
    "                            'function': func,\n",
    "                            'info': {},\n",
    "                            'inputs': ['close'],\n",
    "                            'parameters': {},\n",
    "                            'outputs': [func_name.lower()]\n",
    "                        }\n",
    "\n",
    "        return indicators\n",
    "\n",
    "    def is_indicator_available(self, indicator: str) -> bool:\n",
    "        \"\"\"Check if indicator is available in TALib\"\"\"\n",
    "        return indicator.upper() in self._available_indicators\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def calculate_indicator(self, df: pd.DataFrame, config: IndicatorConfig) -> pd.Series:\n",
    "        \"\"\"Calculate technical indicator with performance optimization and TALib compatibility\"\"\"\n",
    "        indicator_name = config.name.upper()\n",
    "\n",
    "        if not self.is_indicator_available(indicator_name):\n",
    "            raise TAException(f\"Indicator '{indicator_name}' not available in TALib\",\n",
    "                            \"INDICATOR_NOT_FOUND\")\n",
    "\n",
    "        # Create cache key\n",
    "        cache_key = self._create_cache_key(df, config)\n",
    "\n",
    "        if cache_key in self._indicator_cache:\n",
    "            logger.debug(f\"Using cached result for {indicator_name}\")\n",
    "            return self._indicator_cache[cache_key]\n",
    "\n",
    "        try:\n",
    "            func_info = self._available_indicators[indicator_name]\n",
    "            func = func_info['function']\n",
    "\n",
    "            # Prepare parameters\n",
    "            kwargs = self._prepare_parameters(config, func_info)\n",
    "\n",
    "            # Get input data with proper data types\n",
    "            input_data = self._prepare_input_data(df, config, func_info)\n",
    "\n",
    "            # Special handling for indicators that need multiple inputs\n",
    "            if indicator_name == 'ADX':\n",
    "                # ADX needs High, Low, Close\n",
    "                if len(input_data) < 3:\n",
    "                    high_data = df['High'].astype(np.float64).values if 'High' in df.columns else df[config.source_column].astype(np.float64).values\n",
    "                    low_data = df['Low'].astype(np.float64).values if 'Low' in df.columns else df[config.source_column].astype(np.float64).values\n",
    "                    close_data = df['Close'].astype(np.float64).values if 'Close' in df.columns else df[config.source_column].astype(np.float64).values\n",
    "                    result = func(high_data, low_data, close_data, **kwargs)\n",
    "                else:\n",
    "                    result = func(input_data[0], input_data[1], input_data[2], **kwargs)\n",
    "            elif indicator_name == 'MACD':\n",
    "                # MACD typically returns 3 values: macd, signal, histogram\n",
    "                result = func(input_data[0], **kwargs)\n",
    "            elif indicator_name == 'BBANDS':\n",
    "                # BBANDS returns upper, middle, lower\n",
    "                result = func(input_data[0], **kwargs)\n",
    "            else:\n",
    "                # Calculate indicator with proper input handling\n",
    "                with PerformanceProfiler.memory_efficient_processing():\n",
    "                    if len(input_data) == 1:\n",
    "                        result = func(input_data[0], **kwargs)\n",
    "                    elif len(input_data) == 2:\n",
    "                        result = func(input_data[0], input_data[1], **kwargs)\n",
    "                    elif len(input_data) == 3:\n",
    "                        result = func(input_data[0], input_data[1], input_data[2], **kwargs)\n",
    "                    else:\n",
    "                        result = func(*input_data, **kwargs)\n",
    "\n",
    "            # Handle multiple output indicators\n",
    "            if isinstance(result, tuple):\n",
    "                if indicator_name == 'MACD' and len(result) >= 2:\n",
    "                    result = result[0]  # Take MACD line\n",
    "                elif indicator_name == 'BBANDS':\n",
    "                    result = result[1]  # Take middle band (SMA)\n",
    "                else:\n",
    "                    result = result[0]  # Take first output by default\n",
    "\n",
    "            result_series = pd.Series(result, index=df.index)\n",
    "\n",
    "            # Cache result\n",
    "            self._indicator_cache[cache_key] = result_series\n",
    "\n",
    "            return result_series\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"TALib calculation error details: {str(e)}\")\n",
    "            raise TAException(f\"Failed to calculate {indicator_name}: {str(e)}\",\n",
    "                            \"CALCULATION_ERROR\")\n",
    "\n",
    "    def _create_cache_key(self, df: pd.DataFrame, config: IndicatorConfig) -> str:\n",
    "        \"\"\"Create cache key for indicator calculation\"\"\"\n",
    "        data_hash = hash(tuple(df[config.source_column].fillna(0)))\n",
    "        config_hash = hash((config.name, config.period, config.fast_period,\n",
    "                          config.slow_period, config.signal_period))\n",
    "        return f\"{data_hash}_{config_hash}\"\n",
    "\n",
    "    def _prepare_parameters(self, config: IndicatorConfig,\n",
    "                          func_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Prepare parameters for TALib function\"\"\"\n",
    "        kwargs = {}\n",
    "\n",
    "        # Map standard parameters\n",
    "        if config.period is not None:\n",
    "            kwargs['timeperiod'] = config.period\n",
    "        if config.fast_period is not None:\n",
    "            kwargs['fastperiod'] = config.fast_period\n",
    "        if config.slow_period is not None:\n",
    "            kwargs['slowperiod'] = config.slow_period\n",
    "        if config.signal_period is not None:\n",
    "            kwargs['signalperiod'] = config.signal_period\n",
    "\n",
    "        # Add custom parameters\n",
    "        kwargs.update(config.parameters)\n",
    "\n",
    "        return kwargs\n",
    "\n",
    "    def _prepare_input_data(self, df: pd.DataFrame, config: IndicatorConfig,\n",
    "                          func_info: Dict[str, Any]) -> List[np.ndarray]:\n",
    "        \"\"\"Prepare input data based on indicator requirements - ensure float64 for TALib\"\"\"\n",
    "        inputs = func_info.get('inputs', ['close'])\n",
    "        input_data = []\n",
    "\n",
    "        for inp in inputs:\n",
    "            if inp.lower() in ['close', 'real']:\n",
    "                # Ensure float64 for TALib compatibility\n",
    "                data = df[config.source_column].astype(np.float64).values\n",
    "                input_data.append(data)\n",
    "            elif inp.lower() == 'high':\n",
    "                if 'High' in df.columns:\n",
    "                    data = df['High'].astype(np.float64).values\n",
    "                else:\n",
    "                    data = df[config.source_column].astype(np.float64).values\n",
    "                input_data.append(data)\n",
    "            elif inp.lower() == 'low':\n",
    "                if 'Low' in df.columns:\n",
    "                    data = df['Low'].astype(np.float64).values\n",
    "                else:\n",
    "                    data = df[config.source_column].astype(np.float64).values\n",
    "                input_data.append(data)\n",
    "            elif inp.lower() == 'open':\n",
    "                if 'Open' in df.columns:\n",
    "                    data = df['Open'].astype(np.float64).values\n",
    "                else:\n",
    "                    data = df[config.source_column].astype(np.float64).values\n",
    "                input_data.append(data)\n",
    "            elif inp.lower() == 'volume':\n",
    "                if 'Volume' in df.columns:\n",
    "                    data = df['Volume'].astype(np.float64).values\n",
    "                else:\n",
    "                    data = np.ones(len(df), dtype=np.float64)\n",
    "                input_data.append(data)\n",
    "            else:\n",
    "                data = df[config.source_column].astype(np.float64).values\n",
    "                input_data.append(data)\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear indicator calculation cache\"\"\"\n",
    "        self._indicator_cache.clear()\n",
    "        logger.info(\"Indicator cache cleared\")\n",
    "\n",
    "# %%\n",
    "class QueryParser:\n",
    "    \"\"\"Enhanced query parser with better natural language understanding\"\"\"\n",
    "\n",
    "    COMPARISON_PATTERNS = {\n",
    "        r'\\babove\\b': ComparisonType.ABOVE.value,\n",
    "        r'\\bbelow\\b': ComparisonType.BELOW.value,\n",
    "        r'\\bcrossed[\\s_]?up\\b': ComparisonType.CROSSED_UP.value,\n",
    "        r'\\bcrossed[\\s_]?down\\b': ComparisonType.CROSSED_DOWN.value,\n",
    "        r'\\bequals?\\b': ComparisonType.EQUALS.value,\n",
    "        r'\\bgreater[\\s_]?than[\\s_]?or[\\s_]?equal\\b': ComparisonType.GREATER_EQUAL.value,\n",
    "        r'\\bless[\\s_]?than[\\s_]?or[\\s_]?equal\\b': ComparisonType.LESS_EQUAL.value,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def parse_query(cls, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Enhanced query parsing with better pattern matching\"\"\"\n",
    "        operations = []\n",
    "\n",
    "        for line in query.strip().splitlines():\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            operation = cls._parse_line(line)\n",
    "            if operation:\n",
    "                operations.append(operation)\n",
    "\n",
    "        return operations\n",
    "\n",
    "    @classmethod\n",
    "    def _parse_line(cls, line: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a single query line\"\"\"\n",
    "        line_lower = line.lower()\n",
    "\n",
    "        # Find comparison operation\n",
    "        comparison = None\n",
    "        for pattern, comp_type in cls.COMPARISON_PATTERNS.items():\n",
    "            if re.search(pattern, line_lower):\n",
    "                comparison = comp_type\n",
    "                break\n",
    "\n",
    "        if not comparison:\n",
    "            logger.warning(f\"No valid comparison found in: {line}\")\n",
    "            return None\n",
    "\n",
    "        # Split by comparison operation\n",
    "        parts = re.split(r'\\b(?:above|below|crossed[\\s_]?(?:up|down)|equals?|greater[\\s_]?than[\\s_]?or[\\s_]?equal|less[\\s_]?than[\\s_]?or[\\s_]?equal)\\b',\n",
    "                        line, flags=re.IGNORECASE)\n",
    "\n",
    "        if len(parts) < 2:\n",
    "            logger.warning(f\"Malformed query line: {line}\")\n",
    "            return None\n",
    "\n",
    "        column1 = parts[0].strip()\n",
    "        column2 = parts[1].strip()\n",
    "\n",
    "        # Try to convert column2 to numeric\n",
    "        try:\n",
    "            column2 = float(column2)\n",
    "        except ValueError:\n",
    "            pass  # Keep as string\n",
    "\n",
    "        return {\n",
    "            'column1': column1,\n",
    "            'operation': comparison,\n",
    "            'column2': column2,\n",
    "            'original_line': line\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_indicators(query: str) -> List[IndicatorConfig]:\n",
    "        \"\"\"Extract indicator configurations from query\"\"\"\n",
    "        indicators = []\n",
    "        words = re.findall(r'\\b[A-Z_]+_\\d+\\b|\\b[A-Z_]+\\b', query.upper())\n",
    "\n",
    "        for word in words:\n",
    "            if word in ['ABOVE', 'BELOW', 'CROSSED', 'UP', 'DOWN', 'EQUALS']:\n",
    "                continue\n",
    "\n",
    "            if '_' in word:\n",
    "                parts = word.split('_')\n",
    "                if len(parts) >= 2 and parts[1].isdigit():\n",
    "                    indicators.append(IndicatorConfig(\n",
    "                        name=parts[0],\n",
    "                        period=int(parts[1])\n",
    "                    ))\n",
    "            else:\n",
    "                # Check if it's a known indicator\n",
    "                engine = TALibIndicatorEngine()\n",
    "                if engine.is_indicator_available(word):\n",
    "                    indicators.append(IndicatorConfig(name=word))\n",
    "\n",
    "        return list({(ind.name, ind.period): ind for ind in indicators}.values())\n",
    "\n",
    "# %%\n",
    "class BaseComparator(ABC):\n",
    "    \"\"\"Enhanced abstract base class for comparison operations\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Perform the comparison operation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _generate_column_name(self, x: str, y: Union[str, float], operation: str) -> str:\n",
    "        \"\"\"Generate descriptive column name\"\"\"\n",
    "        y_str = str(y).replace('.', '_').replace('-', 'neg')\n",
    "        return f\"{x}_{operation}_{y_str}\"\n",
    "\n",
    "    def _add_constant_column(self, df: pd.DataFrame, name: str, value: float) -> pd.DataFrame:\n",
    "        \"\"\"Add a constant value column efficiently\"\"\"\n",
    "        if name not in df.columns:\n",
    "            df[name] = np.full(len(df), value, dtype=np.float64)\n",
    "        return df\n",
    "\n",
    "# %%\n",
    "class AboveComparator(BaseComparator):\n",
    "    \"\"\"Optimized above comparison with vectorized operations\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        DataValidator.validate_numeric_column(df, x)\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            comparison_array = df[x].values > y\n",
    "        else:\n",
    "            DataValidator.validate_numeric_column(df, y)\n",
    "            comparison_array = df[x].values > df[y].values\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"above\")\n",
    "        df[new_col] = comparison_array.astype(np.int8)  # Use int8 for memory efficiency\n",
    "        return df\n",
    "\n",
    "# %%\n",
    "class BelowComparator(BaseComparator):\n",
    "    \"\"\"Optimized below comparison\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        DataValidator.validate_numeric_column(df, x)\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            comparison_array = df[x].values < y\n",
    "        else:\n",
    "            DataValidator.validate_numeric_column(df, y)\n",
    "            comparison_array = df[x].values < df[y].values\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"below\")\n",
    "        df[new_col] = comparison_array.astype(np.int8)\n",
    "        return df\n",
    "\n",
    "# %%\n",
    "class CrossedUpComparator(BaseComparator):\n",
    "    \"\"\"Optimized crossed up detection with vectorized operations\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        DataValidator.validate_numeric_column(df, x)\n",
    "\n",
    "        x_values = df[x].values\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            diff = x_values - y\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "        else:\n",
    "            DataValidator.validate_numeric_column(df, y)\n",
    "            y_values = df[y].values\n",
    "            diff = x_values - y_values\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "\n",
    "        # Vectorized cross-up detection\n",
    "        crossed_up = (diff > 0) & (diff_prev <= 0)\n",
    "        crossed_up[0] = False  # First element cannot be a cross\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"crossed_up\")\n",
    "        df[new_col] = crossed_up.astype(np.int8)\n",
    "        return df\n",
    "\n",
    "# %%\n",
    "class CrossedDownComparator(BaseComparator):\n",
    "    \"\"\"Optimized crossed down detection\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        DataValidator.validate_numeric_column(df, x)\n",
    "\n",
    "        x_values = df[x].values\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            diff = x_values - y\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "        else:\n",
    "            DataValidator.validate_numeric_column(df, y)\n",
    "            y_values = df[y].values\n",
    "            diff = x_values - y_values\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "\n",
    "        crossed_down = (diff < 0) & (diff_prev >= 0)\n",
    "        crossed_down[0] = False\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"crossed_down\")\n",
    "        df[new_col] = crossed_down.astype(np.int8)\n",
    "        return df\n",
    "\n",
    "# %%\n",
    "class ComparatorFactory:\n",
    "    \"\"\"Enhanced factory with additional comparison types\"\"\"\n",
    "\n",
    "    _comparators: Dict[str, BaseComparator] = {\n",
    "        ComparisonType.ABOVE.value: AboveComparator(),\n",
    "        ComparisonType.BELOW.value: BelowComparator(),\n",
    "        ComparisonType.CROSSED_UP.value: CrossedUpComparator(),\n",
    "        ComparisonType.CROSSED_DOWN.value: CrossedDownComparator(),\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_comparator(cls, operation: str) -> BaseComparator:\n",
    "        \"\"\"Get comparator instance for the given operation\"\"\"\n",
    "        comparator = cls._comparators.get(operation.lower())\n",
    "        if not comparator:\n",
    "            available = list(cls._comparators.keys())\n",
    "            raise TAException(f\"Unsupported operation '{operation}'. Available: {available}\",\n",
    "                            \"UNSUPPORTED_OPERATION\")\n",
    "        return comparator\n",
    "\n",
    "# %%\n",
    "class EnhancedTechnicalAnalyzer:\n",
    "    \"\"\"Main enhanced technical analyzer with TALib integration and performance optimization\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, validate_ohlcv: bool = True):\n",
    "        \"\"\"Initialize with enhanced validation and optimization\"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TAException(\"Input must be a pandas DataFrame\", \"INVALID_INPUT\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise TAException(\"DataFrame cannot be empty\", \"EMPTY_DATAFRAME\")\n",
    "\n",
    "        self._original_df = df.copy()\n",
    "        self._df = df.copy()\n",
    "        self._operations_log: List[AnalysisResult] = []\n",
    "        self._indicator_engine = TALibIndicatorEngine()\n",
    "\n",
    "        # Validate OHLCV data if requested\n",
    "        if validate_ohlcv:\n",
    "            self._ohlcv_validation = DataValidator.validate_ohlcv_data(df)\n",
    "\n",
    "        # Optimize data types\n",
    "        self._optimize_datatypes()\n",
    "\n",
    "        logger.info(f\"Initialized analyzer with DataFrame shape: {self._df.shape}\")\n",
    "        logger.info(f\"Memory usage: {self._df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB\")\n",
    "\n",
    "    def _optimize_datatypes(self):\n",
    "        \"\"\"Optimize DataFrame data types for better performance - keep OHLCV as float64 for TALib\"\"\"\n",
    "        # Keep OHLCV columns as float64 for TALib compatibility\n",
    "        ohlcv_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "        for col in self._df.select_dtypes(include=['float64']).columns:\n",
    "            # Skip OHLCV columns - TALib needs float64\n",
    "            if col in ohlcv_cols:\n",
    "                continue\n",
    "\n",
    "            if self._df[col].dtype == 'float64':\n",
    "                # Check if values can fit in float32\n",
    "                if (self._df[col].min() >= np.finfo(np.float32).min and\n",
    "                    self._df[col].max() <= np.finfo(np.float32).max):\n",
    "                    self._df[col] = self._df[col].astype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the current DataFrame\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def operations_log(self) -> List[AnalysisResult]:\n",
    "        \"\"\"Get log of all operations performed\"\"\"\n",
    "        return self._operations_log\n",
    "\n",
    "    def add_indicator(self, config: IndicatorConfig, column_name: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Add technical indicator to DataFrame\"\"\"\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            result = self._indicator_engine.calculate_indicator(self._df, config)\n",
    "\n",
    "            # Generate column name\n",
    "            if column_name is None:\n",
    "                if config.period:\n",
    "                    column_name = f\"{config.name}_{config.period}\"\n",
    "                else:\n",
    "                    column_name = config.name\n",
    "\n",
    "            self._df[column_name] = result\n",
    "\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "\n",
    "            # Log operation\n",
    "            analysis_result = AnalysisResult(\n",
    "                column_name=column_name,\n",
    "                operation=f\"ADD_INDICATOR_{config.name}\",\n",
    "                success=True,\n",
    "                message=\"Indicator added successfully\",\n",
    "                data=result,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self._operations_log.append(analysis_result)\n",
    "\n",
    "            logger.info(f\"✓ Added indicator {column_name} in {execution_time:.4f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            analysis_result = AnalysisResult(\n",
    "                column_name=column_name or config.name,\n",
    "                operation=f\"ADD_INDICATOR_{config.name}\",\n",
    "                success=False,\n",
    "                message=str(e),\n",
    "                execution_time=0.0\n",
    "            )\n",
    "            self._operations_log.append(analysis_result)\n",
    "            logger.error(f\"✗ Failed to add indicator {config.name}: {e}\")\n",
    "            raise TAException(f\"Failed to add indicator: {e}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def above(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for above comparison\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.ABOVE.value, x, y, new_col)\n",
    "\n",
    "    def below(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for below comparison\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.BELOW.value, x, y, new_col)\n",
    "\n",
    "    def crossed_up(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for crossed up detection\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.CROSSED_UP.value, x, y, new_col)\n",
    "\n",
    "    def crossed_down(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for crossed down detection\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.CROSSED_DOWN.value, x, y, new_col)\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def _execute_comparison(self, operation: str, x: str, y: Union[str, float],\n",
    "                          new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Execute a comparison operation with enhanced error handling\"\"\"\n",
    "        import time\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        try:\n",
    "            comparator = ComparatorFactory.get_comparator(operation)\n",
    "            self._df = comparator.compare(self._df, x, y, new_col)\n",
    "\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            result_col = new_col or comparator._generate_column_name(x, y, operation)\n",
    "\n",
    "            result = AnalysisResult(\n",
    "                column_name=result_col,\n",
    "                operation=f\"{x} {operation} {y}\",\n",
    "                success=True,\n",
    "                message=\"Comparison completed successfully\",\n",
    "                data=self._df[result_col],\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self._operations_log.append(result)\n",
    "            logger.info(f\"✓ {result.operation} -> {result.column_name} ({execution_time:.4f}s)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            result = AnalysisResult(\n",
    "                column_name=\"\",\n",
    "                operation=f\"{x} {operation} {y}\",\n",
    "                success=False,\n",
    "                message=str(e),\n",
    "                execution_time=0.0\n",
    "            )\n",
    "            self._operations_log.append(result)\n",
    "            logger.error(f\"✗ {result.operation}: {result.message}\")\n",
    "            raise TAException(f\"Comparison operation failed: {e}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def execute_query(self, query: str, auto_add_indicators: bool = True) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Execute natural language query with automatic indicator addition\"\"\"\n",
    "\n",
    "        # Extract and add indicators if requested\n",
    "        if auto_add_indicators:\n",
    "            indicators = QueryParser.extract_indicators(query)\n",
    "            for indicator_config in indicators:\n",
    "                try:\n",
    "                    self.add_indicator(indicator_config)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not add indicator {indicator_config.name}: {e}\")\n",
    "\n",
    "        # Parse and execute comparisons\n",
    "        operations = QueryParser.parse_query(query)\n",
    "\n",
    "        for op in operations:\n",
    "            try:\n",
    "                self._execute_comparison(op['operation'], op['column1'], op['column2'])\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to execute query operation {op}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_signals(self, column: str) -> pd.Series:\n",
    "        \"\"\"Get signal series for a specific column\"\"\"\n",
    "        DataValidator.validate_column_exists(self._df.shape, tuple(self._df.columns), column)\n",
    "        return self._df[column]\n",
    "\n",
    "    def get_active_signals(self, column: str, include_index: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Get only rows where signal is active with performance optimization\"\"\"\n",
    "        DataValidator.validate_column_exists(self._df.shape, tuple(self._df.columns), column)\n",
    "\n",
    "        active_mask = self._df[column] == 1\n",
    "\n",
    "        if include_index:\n",
    "            return self._df[active_mask]\n",
    "        else:\n",
    "            return self._df[active_mask].reset_index(drop=True)\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Enhanced summary with performance metrics\"\"\"\n",
    "        summary_data = []\n",
    "        for result in self._operations_log:\n",
    "            summary_data.append({\n",
    "                'Operation': result.operation,\n",
    "                'Column': result.column_name,\n",
    "                'Success': result.success,\n",
    "                'Execution_Time_ms': round(result.execution_time * 1000, 2),\n",
    "                'Active_Signals': result.data.sum() if result.data is not None else 0,\n",
    "                'Signal_Ratio': (result.data.sum() / len(result.data) * 100) if result.data is not None else 0,\n",
    "                'Message': result.message\n",
    "            })\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "    def performance_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive performance report\"\"\"\n",
    "        total_operations = len(self._operations_log)\n",
    "        successful_operations = sum(1 for op in self._operations_log if op.success)\n",
    "        total_execution_time = sum(op.execution_time for op in self._operations_log)\n",
    "\n",
    "        return {\n",
    "            'total_operations': total_operations,\n",
    "            'successful_operations': successful_operations,\n",
    "            'success_rate': (successful_operations / total_operations * 100) if total_operations > 0 else 0,\n",
    "            'total_execution_time': total_execution_time,\n",
    "            'average_execution_time': total_execution_time / total_operations if total_operations > 0 else 0,\n",
    "            'memory_usage_mb': self._df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "            'dataframe_shape': self._df.shape,\n",
    "            'generated_columns': len([col for col in self._df.columns if col not in self._original_df.columns])\n",
    "        }\n",
    "\n",
    "    def reset(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Reset to original DataFrame state with cache clearing\"\"\"\n",
    "        self._df = self._original_df.copy()\n",
    "        self._operations_log.clear()\n",
    "        self._indicator_engine.clear_cache()\n",
    "        logger.info(\"Reset analyzer to original state\")\n",
    "        return self\n",
    "\n",
    "    def optimize_memory(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Optimize memory usage by downcasting data types\"\"\"\n",
    "        original_memory = self._df.memory_usage(deep=True).sum()\n",
    "\n",
    "        # Optimize numeric columns\n",
    "        for col in self._df.select_dtypes(include=['int64']).columns:\n",
    "            if self._df[col].min() >= 0 and self._df[col].max() <= 255:\n",
    "                self._df[col] = self._df[col].astype('uint8')\n",
    "            elif self._df[col].min() >= -128 and self._df[col].max() <= 127:\n",
    "                self._df[col] = self._df[col].astype('int8')\n",
    "            elif self._df[col].min() >= -32768 and self._df[col].max() <= 32767:\n",
    "                self._df[col] = self._df[col].astype('int16')\n",
    "            elif self._df[col].min() >= -2147483648 and self._df[col].max() <= 2147483647:\n",
    "                self._df[col] = self._df[col].astype('int32')\n",
    "\n",
    "        new_memory = self._df.memory_usage(deep=True).sum()\n",
    "        memory_saved = (original_memory - new_memory) / 1024 / 1024\n",
    "\n",
    "        logger.info(f\"Memory optimization saved {memory_saved:.2f}MB\")\n",
    "        return self\n",
    "\n",
    "    def export_signals(self, filename: str, format: str = 'csv') -> bool:\n",
    "        \"\"\"Export signals to file with multiple format support\"\"\"\n",
    "        try:\n",
    "            signal_columns = [col for col in self._df.columns\n",
    "                            if any(op in col.lower() for op in ['above', 'below', 'crossed'])]\n",
    "\n",
    "            export_df = self._df[signal_columns + ['Close']].copy()\n",
    "\n",
    "            if format.lower() == 'csv':\n",
    "                export_df.to_csv(filename, index=True)\n",
    "            elif format.lower() == 'parquet':\n",
    "                export_df.to_parquet(filename, index=True)\n",
    "            elif format.lower() == 'excel':\n",
    "                export_df.to_excel(filename, index=True)\n",
    "            else:\n",
    "                raise TAException(f\"Unsupported export format: {format}\")\n",
    "\n",
    "            logger.info(f\"Signals exported to {filename}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Export failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def backtest_signals(self, signal_column: str, entry_price_column: str = 'Close',\n",
    "                        holding_period: int = 1) -> Dict[str, float]:\n",
    "        \"\"\"Simple backtesting for signal performance\"\"\"\n",
    "        try:\n",
    "            DataValidator.validate_column_exists(self._df.shape, tuple(self._df.columns), signal_column)\n",
    "            DataValidator.validate_column_exists(self._df.shape, tuple(self._df.columns), entry_price_column)\n",
    "\n",
    "            signals = self._df[signal_column]\n",
    "            prices = self._df[entry_price_column]\n",
    "\n",
    "            # Find signal entry points\n",
    "            entry_points = np.where(signals == 1)[0]\n",
    "\n",
    "            if len(entry_points) == 0:\n",
    "                return {'total_signals': 0, 'avg_return': 0, 'win_rate': 0}\n",
    "\n",
    "            returns = []\n",
    "\n",
    "            for entry_idx in entry_points:\n",
    "                exit_idx = min(entry_idx + holding_period, len(prices) - 1)\n",
    "\n",
    "                if exit_idx > entry_idx:\n",
    "                    entry_price = prices.iloc[entry_idx]\n",
    "                    exit_price = prices.iloc[exit_idx]\n",
    "\n",
    "                    if entry_price != 0:  # Avoid division by zero\n",
    "                        ret = (exit_price - entry_price) / entry_price\n",
    "                        returns.append(ret)\n",
    "\n",
    "            if not returns:\n",
    "                return {'total_signals': len(entry_points), 'avg_return': 0, 'win_rate': 0}\n",
    "\n",
    "            returns = np.array(returns)\n",
    "\n",
    "            return {\n",
    "                'total_signals': len(returns),\n",
    "                'avg_return': returns.mean(),\n",
    "                'win_rate': (returns > 0).sum() / len(returns),\n",
    "                'best_return': returns.max(),\n",
    "                'worst_return': returns.min(),\n",
    "                'total_return': returns.sum()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Backtesting failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "# %%\n",
    "def create_analyzer(df: pd.DataFrame, validate_ohlcv: bool = True) -> EnhancedTechnicalAnalyzer:\n",
    "    \"\"\"Factory function to create EnhancedTechnicalAnalyzer instance\"\"\"\n",
    "    return EnhancedTechnicalAnalyzer(df, validate_ohlcv)\n",
    "\n",
    "# Alias for backward compatibility\n",
    "def cabr(df: pd.DataFrame) -> EnhancedTechnicalAnalyzer:\n",
    "    \"\"\"Legacy factory function for compatibility\"\"\"\n",
    "    return EnhancedTechnicalAnalyzer(df)\n",
    "\n",
    "# %%\n",
    "class TradingSignalGenerator:\n",
    "    \"\"\"Advanced signal generation with multiple timeframes and strategies\"\"\"\n",
    "\n",
    "    def __init__(self, analyzer: EnhancedTechnicalAnalyzer):\n",
    "        self.analyzer = analyzer\n",
    "\n",
    "    def generate_trend_following_signals(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Generate comprehensive trend-following signals with better error handling\"\"\"\n",
    "\n",
    "        # Add required indicators with proper error handling\n",
    "        indicators = [\n",
    "            IndicatorConfig(name='EMA', period=21),\n",
    "            IndicatorConfig(name='EMA', period=50),\n",
    "            IndicatorConfig(name='RSI', period=14),\n",
    "            IndicatorConfig(name='MACD', fast_period=12, slow_period=26, signal_period=9),\n",
    "        ]\n",
    "\n",
    "        # Try to add ADX only if OHLC data is available\n",
    "        if all(col in self.analyzer.df.columns for col in ['High', 'Low', 'Close']):\n",
    "            indicators.append(IndicatorConfig(name='ADX', period=14))\n",
    "\n",
    "        for indicator in indicators:\n",
    "            try:\n",
    "                if indicator.period:\n",
    "                    column_name = f\"{indicator.name}_{indicator.period}\"\n",
    "                else:\n",
    "                    column_name = indicator.name\n",
    "                self.analyzer.add_indicator(indicator, column_name)\n",
    "                logger.info(f\"Successfully added {column_name}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not add {indicator.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Generate signals only if indicators were successfully added\n",
    "        try:\n",
    "            available_cols = self.analyzer.df.columns.tolist()\n",
    "\n",
    "            if 'EMA_21' in available_cols:\n",
    "                self.analyzer.above('Close', 'EMA_21', 'bullish_trend')\n",
    "\n",
    "            if all(col in available_cols for col in ['EMA_21', 'EMA_50']):\n",
    "                self.analyzer.above('EMA_21', 'EMA_50', 'ema_bullish')\n",
    "\n",
    "            if 'RSI_14' in available_cols:\n",
    "                self.analyzer.below('RSI_14', 70, 'rsi_not_overbought')\n",
    "                self.analyzer.above('RSI_14', 30, 'rsi_not_oversold')\n",
    "\n",
    "            # Combine signals if available\n",
    "            signal_components = ['bullish_trend', 'ema_bullish', 'rsi_not_overbought']\n",
    "            available_signals = [col for col in signal_components if col in self.analyzer.df.columns]\n",
    "\n",
    "            if len(available_signals) >= 2:  # At least 2 components available\n",
    "                combined_signal = self.analyzer.df[available_signals[0]].astype(bool)\n",
    "                for signal in available_signals[1:]:\n",
    "                    combined_signal = combined_signal & self.analyzer.df[signal].astype(bool)\n",
    "\n",
    "                self.analyzer.df['trend_following_signal'] = combined_signal.astype(int)\n",
    "                logger.info(f\"Created trend following signal with {len(available_signals)} components\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not generate all trend-following signals: {e}\")\n",
    "\n",
    "        return self.analyzer\n",
    "\n",
    "    def generate_mean_reversion_signals(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Generate mean reversion signals with better error handling\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Add RSI first\n",
    "            self.analyzer.add_indicator(IndicatorConfig(name='RSI', period=14), 'RSI_14')\n",
    "\n",
    "            # Try to add Bollinger Bands - they return 3 values\n",
    "            bb_config = IndicatorConfig(name='BBANDS', period=20)\n",
    "\n",
    "            # BBANDS returns tuple (upper, middle, lower)\n",
    "            bb_result = self.analyzer._indicator_engine.calculate_indicator(self.analyzer.df, bb_config)\n",
    "\n",
    "            # For now, we'll manually calculate simple moving average as middle band\n",
    "            self.analyzer.add_indicator(IndicatorConfig(name='SMA', period=20), 'BB_MIDDLE')\n",
    "\n",
    "            # Calculate upper and lower bands manually (2 std dev from SMA)\n",
    "            if 'BB_MIDDLE' in self.analyzer.df.columns:\n",
    "                close_prices = self.analyzer.df['Close']\n",
    "                sma_20 = self.analyzer.df['BB_MIDDLE']\n",
    "                rolling_std = close_prices.rolling(window=20).std()\n",
    "\n",
    "                self.analyzer.df['BB_UPPER'] = sma_20 + (2 * rolling_std)\n",
    "                self.analyzer.df['BB_LOWER'] = sma_20 - (2 * rolling_std)\n",
    "\n",
    "            # Generate mean reversion signals\n",
    "            if 'BB_LOWER' in self.analyzer.df.columns:\n",
    "                self.analyzer.below('Close', 'BB_LOWER', 'oversold_bb')\n",
    "\n",
    "            if 'RSI_14' in self.analyzer.df.columns:\n",
    "                self.analyzer.below('RSI_14', 30, 'oversold_rsi')\n",
    "\n",
    "            # Combine for mean reversion buy signal\n",
    "            signal_components = ['oversold_bb', 'oversold_rsi']\n",
    "            available_signals = [col for col in signal_components if col in self.analyzer.df.columns]\n",
    "\n",
    "            if len(available_signals) >= 1:\n",
    "                combined_signal = self.analyzer.df[available_signals[0]].astype(bool)\n",
    "                for signal in available_signals[1:]:\n",
    "                    combined_signal = combined_signal & self.analyzer.df[signal].astype(bool)\n",
    "\n",
    "                self.analyzer.df['mean_reversion_buy'] = combined_signal.astype(int)\n",
    "                logger.info(f\"Created mean reversion signal with {len(available_signals)} components\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not generate mean reversion signals: {e}\")\n",
    "\n",
    "        return self.analyzer\n",
    "\n",
    "# %%\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Enhanced Enterprise Technical Analysis Framework ===\\n\")\n",
    "\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=1000, freq='D')\n",
    "\n",
    "    # Generate realistic OHLCV data\n",
    "    close_prices = 100 + np.cumsum(np.random.randn(1000) * 0.02)\n",
    "    high_prices = close_prices + np.abs(np.random.randn(1000) * 0.01)\n",
    "    low_prices = close_prices - np.abs(np.random.randn(1000) * 0.01)\n",
    "    open_prices = close_prices + np.random.randn(1000) * 0.005\n",
    "    volumes = 1000000 + np.random.randint(-100000, 100000, 1000)\n",
    "\n",
    "    sample_data = pd.DataFrame({\n",
    "        'DateTime': dates,\n",
    "        'Open': open_prices,\n",
    "        'High': high_prices,\n",
    "        'Low': low_prices,\n",
    "        'Close': close_prices,\n",
    "        'Volume': volumes\n",
    "    })\n",
    "\n",
    "    # Create analyzer\n",
    "    analyzer = create_analyzer(sample_data)\n",
    "\n",
    "    # Demo 1: Fluent interface with automatic indicator addition\n",
    "    print(\"Demo 1: Fluent Interface with TALib Integration\")\n",
    "    query = \"\"\"\n",
    "    Close above EMA_21\n",
    "    RSI_14 below 70\n",
    "    Volume above 1200000\n",
    "    Close crossed_up EMA_50\n",
    "    \"\"\"\n",
    "\n",
    "    analyzer.execute_query(query)\n",
    "    print(analyzer.summary())\n",
    "    print()\n",
    "\n",
    "    # Demo 2: Performance report\n",
    "    print(\"Demo 2: Performance Report\")\n",
    "    perf_report = analyzer.performance_report()\n",
    "    for key, value in perf_report.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()\n",
    "\n",
    "    # Demo 3: Advanced signal generation\n",
    "    print(\"Demo 3: Advanced Signal Generation\")\n",
    "    signal_gen = TradingSignalGenerator(analyzer)\n",
    "    signal_gen.generate_trend_following_signals()\n",
    "\n",
    "    # Demo 4: Backtesting\n",
    "    if 'trend_following_signal' in analyzer.df.columns:\n",
    "        backtest_results = analyzer.backtest_signals('trend_following_signal', holding_period=5)\n",
    "        print(\"Backtest Results:\", backtest_results)\n",
    "\n",
    "    # Demo 5: Memory optimization\n",
    "    print(\"\\nDemo 5: Memory Optimization\")\n",
    "    original_memory = analyzer.df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    analyzer.optimize_memory()\n",
    "    optimized_memory = analyzer.df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"Memory usage: {original_memory:.2f}MB -> {optimized_memory:.2f}MB\")\n",
    "\n",
    "    print(f\"\\nFinal DataFrame shape: {analyzer.df.shape}\")\n",
    "    print(f\"Generated columns: {len([col for col in analyzer.df.columns if col not in sample_data.columns])}\")\n",
    "\n",
    "    # Display available indicators\n",
    "    engine = TALibIndicatorEngine()\n",
    "    print(f\"\\nAvailable TALib indicators: {len(engine._available_indicators)}\")\n",
    "    print(\"Example indicators:\", list(engine._available_indicators.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = create_analyzer(df)\n",
    "\n",
    "# Method 1: Natural language query (automatically adds indicators)\n",
    "query = \"\"\"\n",
    "Close above EMA_21\n",
    "RSI_14 below 70\n",
    "Volume crossed_up 1000000\n",
    "\"\"\"\n",
    "analyzer.execute_query(query)\n",
    "\n",
    "# Method 2: Fluent interface\n",
    "analyzer = (analyzer\n",
    "    .above('Close', 'EMA_50')\n",
    "    .below('RSI_14', 30)\n",
    "    .crossed_up('MACD', 0))\n",
    "\n",
    "# Method 3: Manual indicator addition\n",
    "analyzer.add_indicator(IndicatorConfig(name='STOCH', fast_period=14, slow_period=3))\n",
    "\n",
    "# Get results\n",
    "print(analyzer.summary())\n",
    "print(analyzer.performance_report())\n",
    "\n",
    "# Advanced signal generation\n",
    "signal_gen = TradingSignalGenerator(analyzer)\n",
    "signal_gen.generate_trend_following_signals()\n",
    "\n",
    "# Backtesting\n",
    "results = analyzer.backtest_signals('trend_following_signal', holding_period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTXV1.0",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
